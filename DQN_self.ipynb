{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from utils import play_game, Metric\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metric_Q(policy,Q_table,optimal=False):\n",
    "    N_wins=0\n",
    "    N_losses=0\n",
    "    N=0\n",
    "    Turns = np.array([['X','O']]*250+[['O','X']]*250)\n",
    "    for i in range(500):\n",
    "        np.random.seed()\n",
    "\n",
    "        \n",
    "        if optimal: \n",
    "            player_test = OptimalPlayer(epsilon=0., player=Turns[i,1])\n",
    "        if not optimal:\n",
    "            player_test = OptimalPlayer(epsilon=1., player=Turns[i,1])\n",
    "\n",
    "        player_new = policy(player=Turns[i,0],epsilon=0)\n",
    "        env=TictactoeEnv()\n",
    "        while not env.end:\n",
    "            if env.current_player == player_new.player:\n",
    "                state=get_state(env.grid,player_new)\n",
    "                move = player_new.act(state,env.grid,Q_table)       \n",
    "            else:\n",
    "                move = player_test.act(env.grid)\n",
    "\n",
    "            if not isinstance(move,tuple): \n",
    "                    move=(int(move/3),move%3)\n",
    "            env.step(move, print_grid=False)\n",
    "                \n",
    "        if env.winner==player_new.player:\n",
    "            N_wins+=1\n",
    "        if env.winner==player_test.player:\n",
    "            N_losses+=1\n",
    "        N+=1\n",
    "        env.reset()               \n",
    "    return (N_wins - N_losses)/N\n",
    "\n",
    "class deep_Q_player:\n",
    "    def __init__(self,player='X', epsilon=0):\n",
    "        self.player=player\n",
    "        self.epsilon=epsilon\n",
    "    \n",
    "        \n",
    "    def act(self,state,model):\n",
    "        \n",
    "        action_scores = model(state)\n",
    "        \n",
    "        action = np.random.choice(np.where(action_scores == np.max(action_scores))[0])\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(range(9))\n",
    "        \n",
    "        return action\n",
    "\n",
    "def play_game_self_deepQ(env,model,p1,p2):\n",
    "    R_t=[]\n",
    "    while not env.end:\n",
    "        if env.current_player == p1.player:\n",
    "            state1=grid2tensor(env.grid,p1.player)\n",
    "            action1=p1.act(state1,model)\n",
    "            if env.check_valid():\n",
    "                env.step((int(action1/3), action1 % 3), print_grid=False)\n",
    "                reward1=env.reward(p1.player)\n",
    "                new_state1=grid2tensor(env.grid,p1.player)\n",
    "                R_t.append(state1,action1,reward1,new_state1)\n",
    "            else:\n",
    "                reward1=-1\n",
    "                R_t.append(state1,action1,reward1,None)\n",
    "                break\n",
    "\n",
    "        if env.current_player == p2.player:\n",
    "            state2=grid2tensor(env.grid,p2.player)\n",
    "            action2=p2.act(state2,model)\n",
    "            if env.check_valid():\n",
    "                env.step((int(action2/3), action2 % 3), print_grid=False)\n",
    "                reward2=env.reward(p2.player)\n",
    "                new_state2=grid2tensor(env.grid,p2.player)\n",
    "                R_t.append(state2,action2,reward2,new_state2)\n",
    "            else:\n",
    "                reward2=-1\n",
    "                R_t.append(state2,action2,reward2,None)\n",
    "                break\n",
    "        \n",
    "    return env, R_t\n",
    "\n",
    "def Q_loss(r,max_new_Q_val,Q_val,gamma=.99):\n",
    "        return .5(r + gamma*max_new_Q_val - Q_val).pow(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=.99\n",
    "buff_size=10000\n",
    "batch_size=64\n",
    "lr=5e-4\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN,self).__init__()\n",
    "\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(3*3*2,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,9)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=x.view(18)\n",
    "        x=self.fc(x)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid2tensor(grid,player):\n",
    "    tens=torch.zeros(3,3,2)\n",
    "    grid=torch.from_numpy(grid)\n",
    "    if player=='X':\n",
    "        player_index=torch.nonzero(grid==1,as_tuple=False)\n",
    "        op_index=torch.nonzero(grid==-1,as_tuple=False)\n",
    "    if player=='O':\n",
    "        player_index=torch.nonzero(grid==-1,as_tuple=False)\n",
    "        op_index=torch.nonzero(grid==1,as_tuple=False)\n",
    "    tens[player_index[:,0],player_index[:,1],0]=1\n",
    "    tens[op_index[:,0],op_index[:,1],1]=1\n",
    "    return tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=DQN()\n",
    "R=[]\n",
    "target=model\n",
    "p1=deep_Q_player('X')\n",
    "p2=deep_Q_player('O')\n",
    "#should we first play all games, then train? or do it at the same time?\n",
    "for game in range(nb_games):\n",
    "    env=TictactoeEnv()\n",
    "    env, R_t=play_game_self_deepQ(env,model,p1,p2)\n",
    "    R.extend(R_t)\n",
    "    sample=sample_from(R)\n",
    "    loss=Q_loss(rews,max_new_Q_vals,Q_vals)\n",
    "    if game%target_update_step==0:\n",
    "        target=model\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def m(a):\n",
    "    a=0\n",
    "a=1\n",
    "print(a)\n",
    "m(a)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "316bcdbfefbf4057bea60a74d4981cf3304ddbec1a2ad80e595554335e51b39e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('DLenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
