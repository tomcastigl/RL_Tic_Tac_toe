{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from utils import play_game, Metric\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Helpers ###################################\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN,self).__init__()\n",
    "\n",
    "        self.fc=nn.Sequential(nn.Linear(3*3*2,128),nn.ReLU(),\n",
    "                              nn.Linear(128,128),nn.ReLU(),\n",
    "                              nn.Linear(128,128),nn.ReLU(),\n",
    "                              nn.Linear(128,9))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #x=x.view(-1,18)#x.sum(2).view(-1,9)\n",
    "        x=self.fc(x)\n",
    "        return x\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity,batch_size):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "        self.Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "        self.device='cpu'\n",
    "        self.batch_size=batch_size\n",
    "        self.step = 0\n",
    "        self.accumulated_loss = 0\n",
    "        self.optimizer = None\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(self.Transition(*args))\n",
    "\n",
    "    def sample(self):\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class deep_Q_player:\n",
    "    def __init__(self):\n",
    "        self.player=''\n",
    "        \n",
    "    def act(self,state,model:nn.Module,epsilon:float):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_scores = model(state)\n",
    "        \n",
    "        action = action_scores.max(1)[1].item()\n",
    "        \n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(range(9)).item()\n",
    "        \n",
    "        return action\n",
    "\n",
    "def grid2tensor(grid:np.array,player:str):\n",
    "    \n",
    "    state=np.zeros((3,3,2))\n",
    "    a = 2*(player=='X')-1 #  1 if player='X' and -1 otherwise\n",
    "    \n",
    "    grid1 = np.where(grid==a,1,0)\n",
    "    grid2 = np.where(grid==-a,1,0)\n",
    "    \n",
    "    state[:,:,0]=grid1\n",
    "    state[:,:,1]=grid2\n",
    "    state = torch.tensor(state)\n",
    "    \n",
    "    flatten_arrays = (state[:,:,0].flatten(),state[:,:,1].flatten())\n",
    "    \n",
    "    return torch.cat(flatten_arrays).view(-1,18).float()\n",
    "\n",
    "def test_policy(eps_optimalplayer,q_table=None,verbose=False,DQN_policy_net=None):\n",
    "    \n",
    "    env = TictactoeEnv() # environment\n",
    "    \n",
    "    if DQN_policy_net is None:\n",
    "        assert q_table is not None, \"Provide q_table\"\n",
    "        agent_player = agent(q_table) # agent    \n",
    "        \n",
    "    else:\n",
    "        agent_player=deep_Q_player()\n",
    "    \n",
    "    #-- Holders \n",
    "    wins_count = dict()\n",
    "    wins_count['optimal_player']=0\n",
    "    wins_count['agent']=0\n",
    "    wins_count['draw']=0 \n",
    "    players = dict()\n",
    "    players[None] = 'draw'\n",
    "    turns = np.array(['X','O'])\n",
    "    agent_symbol = None # 'X' or 'O'\n",
    "    optimal_symbol = None\n",
    "    num_illegal_steps = 0\n",
    "    \n",
    "    for episode in range(500):\n",
    "        \n",
    "        env.reset()\n",
    "        np.random.seed(episode) \n",
    "        \n",
    "        if episode < 250 :\n",
    "            agent_symbol = turns[0]\n",
    "            optimal_symbol = turns[1]\n",
    "            \n",
    "        else:\n",
    "            agent_symbol = turns[1]\n",
    "            optimal_symbol = turns[0]\n",
    "        \n",
    "        player_opt = OptimalPlayer(epsilon=eps_optimalplayer,player=optimal_symbol)\n",
    "        players[optimal_symbol]=(player_opt,'optimal_player')\n",
    "        players[agent_symbol]=(agent_player,'agent')\n",
    "        \n",
    "        for j in range(9):    \n",
    "            \n",
    "            #-- Get turn\n",
    "            turn = env.current_player\n",
    "            \n",
    "            #-- observe grid\n",
    "            grid,end,_ = env.observe() \n",
    "            \n",
    "            #-- Play\n",
    "            current_player, _ = players[turn]\n",
    "            \n",
    "            #-- Playing with DQN-agent\n",
    "            if DQN_policy_net is not None and turn==agent_symbol:\n",
    "                state = grid2tensor(grid,agent_symbol)\n",
    "                move = current_player.act(state,DQN_policy_net,0)\n",
    "                \n",
    "                try:\n",
    "                    env.step(move,print_grid=False)\n",
    "                    \n",
    "                except ValueError:\n",
    "                    env.end = True\n",
    "                    env.winner = optimal_symbol\n",
    "                    num_illegal_steps += 1\n",
    "   \n",
    "            else:\n",
    "                move = current_player.act(grid)  \n",
    "                env.step(move,print_grid=False)\n",
    "        \n",
    "            #-- Chek that the game has finished\n",
    "            if env.end :\n",
    "                if env.winner is not None :\n",
    "                    _,winner = players[env.winner]\n",
    "                    wins_count[winner] = wins_count[winner] + 1\n",
    "                else :\n",
    "                    wins_count['draw'] = wins_count['draw'] + 1\n",
    "                \n",
    "                break\n",
    "    \n",
    "    M = (wins_count['agent']-wins_count['optimal_player'])/500\n",
    "    \n",
    "    if verbose :\n",
    "        string =\"M_rand\"\n",
    "        if eps_optimalplayer < 1:\n",
    "            string = \"M_opt\"    \n",
    "        print(string+\" : \",M)\n",
    "        print(wins_count,'\\n')\n",
    "        print('Number of illegal steps',num_illegal_steps)\n",
    "\n",
    "    \n",
    "    return M,num_illegal_steps\n",
    "\n",
    "def update_policy(policy_net:nn.Module,\n",
    "                  target_net:nn.Module,\n",
    "                  memory:ReplayMemory,\n",
    "                  criterion=nn.SmoothL1Loss(),# F.huber_loss,\n",
    "                  gamma=0.99,\n",
    "                  online_update=False,online_state_action_reward=(None,None,None,None)):\n",
    "    \n",
    "    \n",
    "    if online_update :\n",
    "        #assert None not in online_state_action_reward,'provide these values.'\n",
    "        \n",
    "        #-- Compute Q values\n",
    "        state,next_state,action,reward = online_state_action_reward\n",
    "        state=state.to(memory.device)\n",
    "        state_action_values = policy_net(state)[:,action] # take Q(state,action)\n",
    "        \n",
    "        next_state_values=torch.tensor([0.0])\n",
    "        if next_state is not None:\n",
    "            next_state = next_state.to(memory.device)\n",
    "            next_state_values = target_net(next_state).max(1)[0].detach() # take max Q(state',action')\n",
    "        \n",
    "        #-- Compute target\n",
    "        target = reward + gamma*next_state_values\n",
    "        \n",
    "        #-- Update gradients\n",
    "        memory.optimizer.zero_grad()\n",
    "        loss = criterion(state_action_values,target)\n",
    "        loss.backward()\n",
    "        memory.optimizer.step\n",
    "        memory.step += 1\n",
    "\n",
    "        #-- Log\n",
    "        #wandb.log({'loss':loss.item(),'reward':reward,'Step':memory.step})\n",
    "\n",
    "    else:\n",
    "        if len(memory) < memory.batch_size:\n",
    "            return False\n",
    "        \n",
    "        #-- Sample Transitions\n",
    "        transitions = memory.sample()\n",
    "        \n",
    "        #-- GetTransition of batch-arrays\n",
    "        batch = memory.Transition(*zip(*transitions))\n",
    "        \n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=memory.device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        \n",
    "        state_batch = torch.cat(batch.state).to(memory.device)\n",
    "        action_batch = torch.cat(batch.action).to(memory.device)\n",
    "        reward_batch = torch.cat(batch.reward).to(memory.device)\n",
    "\n",
    "        #-- Compute Q values\n",
    "        memory.optimizer.zero_grad()\n",
    "        state_action_values = policy_net(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "        \n",
    "        #-- Compute target\n",
    "        next_state_values = torch.zeros(memory.batch_size,device=memory.device)\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "        target = reward_batch + gamma*next_state_values\n",
    "        \n",
    "        #-- Update gradients\n",
    "        loss = criterion(state_action_values,target.unsqueeze(1))#,reduction='mean')\n",
    "        loss.backward()\n",
    "        #for p in policy_net.parameters():\n",
    "        #    p.grad.data.clamp_(-1,1)\n",
    "        memory.optimizer.step()\n",
    "        memory.step += 1\n",
    "\n",
    "        #-- Log\n",
    "        #wandb.log({'loss':loss,'mean_batch_reward':reward_batch.float().mean(),'Step':memory.step})\n",
    "    \n",
    "    memory.accumulated_loss += loss.item()\n",
    "    return True # loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(epsilon,num_episodes:int,\n",
    "                    env:TictactoeEnv,\n",
    "                    path_save:str,\n",
    "                    eps_opt=0.5,gamma=0.99,\n",
    "                    render=False,test=False,online_update=False,wandb_tag=\"DQN\"):\n",
    "    #-- agent\n",
    "    agent = deep_Q_player()\n",
    "    \n",
    "    #-- Initialize Q networks\n",
    "    policy_net = DQN()\n",
    "    target_net = DQN()\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    #-- Initialize hyperparameters\n",
    "    batch_size=64\n",
    "    gamma=0.99\n",
    "    lr=5e-4\n",
    "    memory = ReplayMemory(10000,batch_size)\n",
    "    memory.optimizer = optim.Adam(policy_net.parameters(),lr=lr) # optimizer\n",
    "    args={'gamma':gamma,'batch_size':batch_size,'replay_buffer':int(1e4),'lr':lr,'eps_opt':eps_opt,'online_update':online_update}\n",
    "    policy_net.to(memory.device)\n",
    "    target_net.to(memory.device)\n",
    "\n",
    "    #-- wandb init\n",
    "    #wandb.init(tags=[wandb_tag],project='ANN',entity='fadelmamar', \n",
    "    #           name='DQN_learnFromXpert', config=args)\n",
    "    \n",
    "    #-- Holder \n",
    "    wins_count = dict()\n",
    "    wins_count['optimal_player']=0\n",
    "    wins_count['agent']=0\n",
    "    wins_count['draw']=0\n",
    "    players = dict()\n",
    "    M_opts = list()\n",
    "    M_rands = list()\n",
    "    accumulate_reward = 0\n",
    "    agent_mean_rewards = [0]*int(num_episodes//250)\n",
    "    num_illegal_actions = 0\n",
    "    turns = np.array(['X','O'])\n",
    "    \n",
    "    for episode in range(1,num_episodes+1):\n",
    "        \n",
    "        #wandb.log({'episode':episode,'epsilon_greedy':epsilon(episode)})\n",
    "        \n",
    "        if episode % 250 == 0 :\n",
    "            agent_mean_rewards[int(episode//250)-1] = accumulate_reward/250\n",
    "            #wandb.log({'mean_reward':accumulate_reward/250,'mean_loss':memory.accumulated_loss/250})\n",
    "            accumulate_reward = 0 # reset\n",
    "            memory.accumulated_loss = 0 # reset\n",
    "            \n",
    "            if test:\n",
    "                M_opt,num_illegal_opt = test_policy(0,q_table=None,DQN_policy_net=policy_net,verbose=False)\n",
    "                M_rand,num_illegal_rand = test_policy(1,q_table=None,DQN_policy_net=policy_net,verbose=False)\n",
    "                M_opts.append(M_opt)\n",
    "                M_rands.append(M_rand)\n",
    "                #wandb.log({'M_opt':M_opt,'M_rand':M_rand,\n",
    "                #           'num_illegal_opt':num_illegal_opt,\n",
    "                #           'num_illegal_rand':num_illegal_rand})\n",
    "            \n",
    "        env.reset()\n",
    "        #-- Permuting player every 2 games\n",
    "        if episode % 2 == 0 :\n",
    "            turns[0] = 'X'\n",
    "            turns[1] = 'O'\n",
    "        else:\n",
    "            turns[0] = 'O'\n",
    "            turns[1] = 'X'\n",
    "        \n",
    "        player_opt = OptimalPlayer(epsilon=eps_opt,player=turns[0])\n",
    "        agent_learner = turns[1]\n",
    "        players[turns[0]]='optimal_player'\n",
    "        players[turns[1]]='agent'\n",
    "        \n",
    "        #--\n",
    "        current_state = None\n",
    "        A = None # action\n",
    "        \n",
    "        for j in range(9):\n",
    "  \n",
    "            #-- Agent plays \n",
    "            if env.current_player == turns[1] :\n",
    "                \n",
    "                current_state = grid2tensor(env.observe()[0],agent_learner)                          \n",
    "                A = agent.act(current_state, policy_net,epsilon(episode))  #----- Choose action A with epsilon greedy\n",
    "                #wandb.log({'action':A})  \n",
    "                \n",
    "                #----- Take action A\n",
    "                try :\n",
    "                    _,_,_ = env.step(A,print_grid=False)\n",
    "                                        \n",
    "                #----- End game when agent moves illegaly\n",
    "                except ValueError :                                \n",
    "                    num_illegal_actions += 1\n",
    "                    #wandb.log({'num_illegal_actions':num_illegal_actions})\n",
    "                    env.end = True #-- Terminating game\n",
    "                    env.winner = turns[0] # optimal player\n",
    "                    \n",
    "            #-- Optimal player plays \n",
    "            if not env.end :\n",
    "                \n",
    "                grid,end,_ = env.observe() #-- observe grid\n",
    "                move = player_opt.act(grid) #-- get move\n",
    "                env.step(move,print_grid=False) # optimal player takes a move\n",
    "  \n",
    "                #-- Update agent and Replay buffer\n",
    "                if current_state is not None :   \n",
    "                    next_state = grid2tensor(env.observe()[0],agent_learner)    \n",
    "                    agent_reward = env.reward(agent_learner)\n",
    "                    \n",
    "                    if not env.end : \n",
    "                        memory.push(current_state, torch.tensor([A]), next_state, torch.tensor([agent_reward]))\n",
    "                        \n",
    "                    if online_update:\n",
    "                        update_policy(policy_net,target_net,memory,gamma=gamma,\n",
    "                                      online_update=True,online_state_action_reward=(current_state,next_state,A,agent_reward))\n",
    "\n",
    "            #-- Update policy offline if applicable\n",
    "            if online_update == False :\n",
    "                success = update_policy(policy_net,target_net, memory,gamma=gamma, online_update=False)\n",
    "\n",
    "            #-- Chek that the game has finished\n",
    "            if env.end :\n",
    "                agent_reward = env.reward(agent_learner)\n",
    "                memory.push(current_state, torch.tensor([A]), None, torch.tensor([agent_reward])) #-- Store in Replay buffer\n",
    "                \n",
    "                if online_update:\n",
    "                    update_policy(policy_net,target_net,memory,gamma=gamma,\n",
    "                                  online_update=True,online_state_action_reward=(current_state,None,A,agent_reward))\n",
    "                #-- Logging\n",
    "                if env.winner is not None :\n",
    "                    winner = players[env.winner]\n",
    "                    wins_count[winner] = wins_count[winner] + 1            \n",
    "                else :\n",
    "                    wins_count['draw'] = wins_count['draw'] + 1  \n",
    "                accumulate_reward += agent_reward\n",
    "                #wandb.log({'accumulated_reward':accumulate_reward,\n",
    "                #           'reward':agent_reward})\n",
    "                #wandb.log(wins_count)\n",
    "                #-- Render\n",
    "                if render : \n",
    "                    print(f\"Episode {episode} ; Winner is {winner}.\")\n",
    "                    env.render()\n",
    "                    \n",
    "                break # stop for-loop\n",
    "        \n",
    "        #-- Log results            \n",
    "        if episode % 5000 == 0 :\n",
    "            print(f\"\\nEpisode : {episode}\")\n",
    "            print(wins_count)\n",
    "            \n",
    "        #-- Upddate target network\n",
    "        if episode % 500 == 0:\n",
    "            target_net.load_state_dict(deepcopy(policy_net.state_dict()))\n",
    "            target_net.eval()\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "    return wins_count,agent_mean_rewards,M_opts,M_rands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Q.11 & Q.12\n",
    "eps_1=lambda x : 0.3\n",
    "if False:\n",
    "    test=False\n",
    "    for do in [False,True]:\n",
    "        env = TictactoeEnv()\n",
    "        wins_count,agent_mean_rewards,M_opts,M_rands = deep_q_learning(epsilon=eps_1,num_episodes=int(20e3),\n",
    "                                                                          eps_opt=0.5,env=env,path_save=None,\n",
    "                                                                          gamma=0.99,render=False,test=test,\n",
    "                                                                          wandb_tag=\"V3\",online_update=do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Q.13\n",
    "eps_min=0.1\n",
    "eps_max=0.8\n",
    "if False :\n",
    "    env = TictactoeEnv()\n",
    "    test=True\n",
    "    for N_star in [1,10e3,20e3,30e3,40e3]:\n",
    "        print('-'*20,' N_star : ',N_star,'-'*20)\n",
    "        eps_2=lambda x : max([eps_min,eps_max*(1-x/N_star)])\n",
    "        wins_count,agent_mean_rewards,M_opts,M_rands = deep_q_learning(epsilon=eps_2,num_episodes=int(20e3),\n",
    "                                                                          eps_opt=0.5,env=env,path_save=None,\n",
    "                                                                          gamma=0.99,render=False,test=test,\n",
    "                                                                          wandb_tag=f\"V3--{int(N_star)}\",online_update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Q.14\n",
    "eps_min=0.1\n",
    "eps_max=0.8\n",
    "if True :\n",
    "    env = TictactoeEnv()\n",
    "    test=True\n",
    "    N_star=1 # best N_star from Q.13\n",
    "    eps_2=lambda x : max([eps_min,eps_max*(1-x/N_star)])\n",
    "    for eps_opt in [0,0.25,0.5,0.75,1]:\n",
    "        print('-'*20,' eps_opt : ',eps_opt,'-'*20)\n",
    "        wins_count,agent_mean_rewards,M_opts,M_rands = deep_q_learning(epsilon=eps_2,num_episodes=int(20e3),\n",
    "                                                                          eps_opt=eps_opt,env=env,path_save=None,\n",
    "                                                                          gamma=0.99,render=False,test=test,\n",
    "                                                                          wandb_tag=f\"V3--eps_opt:{eps_opt}\",online_update=False)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metric_Q(policy,Q_table,optimal=False):\n",
    "    N_wins=0\n",
    "    N_losses=0\n",
    "    N=0\n",
    "    Turns = np.array([['X','O']]*250+[['O','X']]*250)\n",
    "    for i in range(500):\n",
    "        np.random.seed()\n",
    "\n",
    "        \n",
    "        if optimal: \n",
    "            player_test = OptimalPlayer(epsilon=0., player=Turns[i,1])\n",
    "        if not optimal:\n",
    "            player_test = OptimalPlayer(epsilon=1., player=Turns[i,1])\n",
    "\n",
    "        player_new = policy(player=Turns[i,0],epsilon=0)\n",
    "        env=TictactoeEnv()\n",
    "        while not env.end:\n",
    "            if env.current_player == player_new.player:\n",
    "                state=get_state(env.grid,player_new)\n",
    "                move = player_new.act(state,env.grid,Q_table)       \n",
    "            else:\n",
    "                move = player_test.act(env.grid)\n",
    "\n",
    "            if not isinstance(move,tuple): \n",
    "                    move=(int(move/3),move%3)\n",
    "            env.step(move, print_grid=False)\n",
    "                \n",
    "        if env.winner==player_new.player:\n",
    "            N_wins+=1\n",
    "        if env.winner==player_test.player:\n",
    "            N_losses+=1\n",
    "        N+=1\n",
    "        env.reset()               \n",
    "    return (N_wins - N_losses)/N\n",
    "\n",
    "class deep_Q_player:\n",
    "    def __init__(self,player='X', epsilon=0):\n",
    "        self.player=player\n",
    "        self.epsilon=epsilon\n",
    "    \n",
    "        \n",
    "    def act(self,state,model):\n",
    "        \n",
    "        action_scores = model(state)\n",
    "        \n",
    "        action = np.random.choice(np.where(action_scores == np.max(action_scores))[0])\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(range(9))\n",
    "        \n",
    "        return action\n",
    "\n",
    "def play_game_self_deepQ(env,model,p1,p2):\n",
    "    R_t=[]\n",
    "    while not env.end:\n",
    "        if env.current_player == p1.player:\n",
    "            state1=grid2tensor(env.grid,p1.player)\n",
    "            action1=p1.act(state1,model)\n",
    "            if env.check_valid():\n",
    "                env.step((int(action1/3), action1 % 3), print_grid=False)\n",
    "                reward1=env.reward(p1.player)\n",
    "                new_state1=grid2tensor(env.grid,p1.player)\n",
    "                R_t.append(state1,action1,reward1,new_state1)\n",
    "            else:\n",
    "                reward1=-1\n",
    "                R_t.append(state1,action1,reward1,None)\n",
    "                break\n",
    "\n",
    "        if env.current_player == p2.player:\n",
    "            state2=grid2tensor(env.grid,p2.player)\n",
    "            action2=p2.act(state2,model)\n",
    "            if env.check_valid():\n",
    "                env.step((int(action2/3), action2 % 3), print_grid=False)\n",
    "                reward2=env.reward(p2.player)\n",
    "                new_state2=grid2tensor(env.grid,p2.player)\n",
    "                R_t.append(state2,action2,reward2,new_state2)\n",
    "            else:\n",
    "                reward2=-1\n",
    "                R_t.append(state2,action2,reward2,None)\n",
    "                break\n",
    "        \n",
    "    return env, R_t\n",
    "\n",
    "def Q_loss(r,max_new_Q_val,Q_val,gamma=.99):\n",
    "        return .5(r + gamma*max_new_Q_val - Q_val).pow(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=.99\n",
    "buff_size=10000\n",
    "batch_size=64\n",
    "lr=5e-4\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN,self).__init__()\n",
    "\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(3*3*2,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,9)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=x.view(18)\n",
    "        x=self.fc(x)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid2tensor(grid,player):\n",
    "    tens=torch.zeros(3,3,2)\n",
    "    grid=torch.from_numpy(grid)\n",
    "    if player=='X':\n",
    "        player_index=torch.nonzero(grid==1,as_tuple=False)\n",
    "        op_index=torch.nonzero(grid==-1,as_tuple=False)\n",
    "    if player=='O':\n",
    "        player_index=torch.nonzero(grid==-1,as_tuple=False)\n",
    "        op_index=torch.nonzero(grid==1,as_tuple=False)\n",
    "    tens[player_index[:,0],player_index[:,1],0]=1\n",
    "    tens[op_index[:,0],op_index[:,1],1]=1\n",
    "    return tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=DQN()\n",
    "R=[]\n",
    "target=model\n",
    "p1=deep_Q_player('X')\n",
    "p2=deep_Q_player('O')\n",
    "#should we first play all games, then train? or do it at the same time?\n",
    "for game in range(nb_games):\n",
    "    env=TictactoeEnv()\n",
    "    env, R_t=play_game_self_deepQ(env,model,p1,p2)\n",
    "    R.extend(R_t)\n",
    "    sample=sample_from(R)\n",
    "    loss=Q_loss(rews,max_new_Q_vals,Q_vals)\n",
    "    if game%target_update_step==0:\n",
    "        target=model\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def m(a):\n",
    "    a=0\n",
    "a=1\n",
    "print(a)\n",
    "m(a)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "316bcdbfefbf4057bea60a74d4981cf3304ddbec1a2ad80e595554335e51b39e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
