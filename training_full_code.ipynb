{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from utils import play_game, Metric\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import itertools\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple, deque\n",
    "from copy import deepcopy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tabular Q-learning\n",
    "The implementations of the the learning from experts agent and self-learning one were done separately. We therefore have different helper functions doing the same job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#-------------------------------------------- Helpers for learning from experts\n",
    "def play_game(p1,p2):\n",
    "    env=TictactoeEnv()\n",
    "    grid, _, __ = env.observe()\n",
    "    for j in range(9):\n",
    "        if env.current_player == p1.player:\n",
    "            move = p1.act(grid)\n",
    "        else:\n",
    "            move = p2.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "        if end:\n",
    "            return env\n",
    "            break\n",
    "\n",
    "def Metric(policy,optimal=False):\n",
    "    N_wins=0\n",
    "    N_losses=0\n",
    "    N=0\n",
    "    for _ in range(5):\n",
    "        np.random.seed()\n",
    "        for i in range(500):\n",
    "            Turns = np.array([['X','O']]*250+[['O','X']]*250)\n",
    "            if optimal:\n",
    "                player_test = OptimalPlayer(epsilon=0., player=Turns[i,1])\n",
    "            if not optimal:\n",
    "                player_test = OptimalPlayer(epsilon=1., player=Turns[i,1])\n",
    "\n",
    "            player_new = policy(player=Turns[i,0])\n",
    "            env_end= play_game(player_new,player_test)\n",
    "            if env_end.winner==player_new.player:\n",
    "                N_wins+=1\n",
    "            if env_end.winner==player_test.player:\n",
    "                N_losses+=1\n",
    "            N+=1\n",
    "\n",
    "    return (N_wins - N_losses)/N\n",
    "\n",
    "############################ Q-Learning from experts\n",
    "def get_hash(state:np.array)->str:    \n",
    "    return str(state.flatten(order='C'))\n",
    "\n",
    "def get_available_positions(state:np.array):\n",
    "      \n",
    "    flatten_array = state.flatten(order='C')\n",
    "    indices = np.argwhere(np.abs(flatten_array)<1).T\n",
    "    \n",
    "    return indices.flatten()\n",
    "\n",
    "def get_best_action(availables_positions:np.array,array_of_q_values:np.array):\n",
    "    \n",
    "    A = np.argmax(array_of_q_values,axis=1)[0]\n",
    "    #print(availables_positions)\n",
    "    \n",
    "    if  A in availables_positions :\n",
    "        return A.item()\n",
    "    \n",
    "    else :\n",
    "        A = availables_positions[0]\n",
    "        for i in availables_positions :\n",
    "            if array_of_q_values[:,i]>array_of_q_values[:,A]:\n",
    "                A = i\n",
    "        return A.item()\n",
    "\n",
    "def get_action(epsilon,current_state,q_table):\n",
    "    \n",
    "    availables_positions = get_available_positions(current_state)\n",
    "    rnd = np.random.uniform(0,1)\n",
    "    current_state_hash = get_hash(current_state)\n",
    "    if rnd > epsilon :\n",
    "        A = get_best_action(availables_positions,q_table[current_state_hash]) # best move among possible moves        \n",
    "    else:\n",
    "        np.random.shuffle(availables_positions) # shuffle\n",
    "        A = availables_positions[0] # take first\n",
    "        A = A.item()\n",
    "    \n",
    "    return A\n",
    "    \n",
    "def get_max_Q(q_table:dict,state:np.array):\n",
    "        \n",
    "    hash_ = get_hash(state)\n",
    "    max_q = 0.0\n",
    "    \n",
    "    if hash_ in q_table.keys():\n",
    "        max_q = np.max(q_table[hash_],axis=1)\n",
    "        \n",
    "    return max_q\n",
    "    \n",
    "def update_player(q_table,current_state,next_state,A,alpha,reward,gamma):\n",
    "    \n",
    "    current_state_hash = get_hash(current_state)\n",
    "    \n",
    "    if next_state is not None :\n",
    "        #----- Update q_table\n",
    "        array_of_q_values = q_table[current_state_hash]\n",
    "        q_value = array_of_q_values[:,A]\n",
    "        updated_q_value = q_value + alpha*(reward + gamma*get_max_Q(q_table,next_state) - q_value)\n",
    "        array_of_q_values[:,A] = updated_q_value # update Q(S,A)\n",
    "        q_table[current_state_hash] = array_of_q_values # store updated Q(S,A)\n",
    "    \n",
    "    else:\n",
    "        #----- Update q_table\n",
    "        array_of_q_values = q_table[current_state_hash]\n",
    "        q_value = array_of_q_values[:,A]\n",
    "        updated_q_value = q_value + alpha*(reward + gamma*0.0 - q_value)\n",
    "        array_of_q_values[:,A] = updated_q_value # update Q(S,A)\n",
    "        q_table[current_state_hash] = array_of_q_values # store updated Q(S,A)\n",
    "    \n",
    "    return q_table\n",
    "\n",
    "class agent:\n",
    "    \n",
    "    def __init__(self,q_table):\n",
    "        self.q_table = q_table\n",
    "        \n",
    "    def act(self,grid):\n",
    "        \n",
    "        grid_hash = get_hash(grid)\n",
    "        availables_positions = get_available_positions(grid)\n",
    "        \n",
    "        #-- for states in the q_table\n",
    "        if grid_hash in self.q_table.keys():\n",
    "            A = get_best_action(availables_positions,self.q_table[grid_hash])\n",
    "        \n",
    "        #-- For any other state not discovered, any available action can be taken\n",
    "        else :\n",
    "            np.random.shuffle(availables_positions)\n",
    "            A = availables_positions[0].item()\n",
    "            \n",
    "        return A\n",
    "\n",
    "def test_policy(eps_optimalplayer,q_table=None,verbose=False,DQN_policy_net=None):\n",
    "    \n",
    "    env = TictactoeEnv() # environment\n",
    "    \n",
    "    if DQN_policy_net is None:\n",
    "        assert q_table is not None, \"Provide q_table\"\n",
    "        agent_player = agent(q_table) # agent    \n",
    "        \n",
    "    else:\n",
    "        agent_player=deep_Q_player()\n",
    "    \n",
    "    #-- Holders \n",
    "    wins_count = dict()\n",
    "    wins_count['optimal_player']=0\n",
    "    wins_count['agent']=0\n",
    "    wins_count['draw']=0 \n",
    "    players = dict()\n",
    "    players[None] = 'draw'\n",
    "    turns = np.array(['X','O'])\n",
    "    agent_symbol = None # 'X' or 'O'\n",
    "    optimal_symbol = None\n",
    "    num_illegal_steps = 0\n",
    "    \n",
    "    for episode in range(500):\n",
    "        \n",
    "        env.reset()\n",
    "        np.random.seed(episode) \n",
    "        \n",
    "        if episode < 250 :\n",
    "            agent_symbol = turns[0]\n",
    "            optimal_symbol = turns[1]\n",
    "            \n",
    "        else:\n",
    "            agent_symbol = turns[1]\n",
    "            optimal_symbol = turns[0]\n",
    "        \n",
    "        player_opt = OptimalPlayer(epsilon=eps_optimalplayer,player=optimal_symbol)\n",
    "        players[optimal_symbol]=(player_opt,'optimal_player')\n",
    "        players[agent_symbol]=(agent_player,'agent')\n",
    "        \n",
    "        for j in range(9):    \n",
    "            \n",
    "            #-- Get turn\n",
    "            turn = env.current_player\n",
    "            \n",
    "            #-- observe grid\n",
    "            grid,end,_ = env.observe() \n",
    "            \n",
    "            #-- Play\n",
    "            current_player, _ = players[turn]\n",
    "            \n",
    "            #-- Playing with DQN-agent\n",
    "            if DQN_policy_net is not None and turn==agent_symbol:\n",
    "                state = grid2tensor(grid,agent_symbol)\n",
    "                move = current_player.act(state,DQN_policy_net,0)\n",
    "                \n",
    "                try:\n",
    "                    env.step(move,print_grid=False)\n",
    "                    \n",
    "                except ValueError:\n",
    "                    env.end = True\n",
    "                    env.winner = optimal_symbol\n",
    "                    num_illegal_steps += 1\n",
    "   \n",
    "            else:\n",
    "                move = current_player.act(grid)  \n",
    "                env.step(move,print_grid=False)\n",
    "        \n",
    "            #-- Chek that the game has finished\n",
    "            if env.end :\n",
    "                if env.winner is not None :\n",
    "                    _,winner = players[env.winner]\n",
    "                    wins_count[winner] = wins_count[winner] + 1\n",
    "                else :\n",
    "                    wins_count['draw'] = wins_count['draw'] + 1\n",
    "                \n",
    "                break\n",
    "    \n",
    "    M = (wins_count['agent']-wins_count['optimal_player'])/500\n",
    "    \n",
    "    if verbose :\n",
    "        string =\"M_rand\"\n",
    "        if eps_optimalplayer < 1:\n",
    "            string = \"M_opt\"    \n",
    "        print(string+\" : \",M)\n",
    "        print(wins_count,'\\n')\n",
    "        print('Number of illegal steps',num_illegal_steps)\n",
    "\n",
    "    \n",
    "    return M,num_illegal_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(epsilon,num_episodes:int,env:TictactoeEnv,path_save:str,\n",
    "               eps_opt=0.5,alpha=0.05,gamma=0.99,render=False,test=False,tag='',wand_name='Tabular-Q'):\n",
    "    \n",
    "    q_table = dict()\n",
    "    turns = np.array(['X','O'])\n",
    "    \n",
    "    #-- Holder \n",
    "    wins_count = dict()\n",
    "    wins_count['optimal_player']=0\n",
    "    wins_count['agent']=0\n",
    "    wins_count['draw']=0\n",
    "    players = dict()\n",
    "    M_opts = list()\n",
    "    M_rands = list()\n",
    "    accumulate_reward = 0\n",
    "    agent_mean_rewards = [0]*int(num_episodes//250)\n",
    "    \n",
    "    #-- Init\n",
    "    #wandb.init(tags=[tag],project='ANN',entity='fadelmamar', \n",
    "    #           name=wand_name, config={'alpha':alpha,'gamma':gamma,'eps_opt':eps_opt})\n",
    "    \n",
    "    for episode in range(1,num_episodes+1):\n",
    "        \n",
    "        if episode % 250 == 0 :\n",
    "            agent_mean_rewards[int(episode//250)-1] = accumulate_reward/250\n",
    "            #wandb.log({'average_reward':accumulate_reward/250})\n",
    "            accumulate_reward = 0 # reset\n",
    "\n",
    "            if test:\n",
    "                M_opt = test_policy(0,q_table=q_table,verbose=False)\n",
    "                M_rand = test_policy(1,q_table=q_table,verbose=False)\n",
    "                M_opts.append(M_opt)\n",
    "                M_rands.append(M_rand)\n",
    "                #wandb.log({'M_opt':M_opt,'M_rand':M_rand})\n",
    "                    \n",
    "        env.reset()     \n",
    "        #-- Permuting player every 2 games\n",
    "        if episode % 2 == 0:\n",
    "            turns[0] = 'X'\n",
    "            turns[1] = 'O'\n",
    "        else:\n",
    "            turns[0] = 'O'\n",
    "            turns[1] = 'X'\n",
    "        \n",
    "        player_opt = OptimalPlayer(epsilon=eps_opt,player=turns[0])\n",
    "        agent_learner = turns[1]       \n",
    "        players[turns[0]]='optimal_player'\n",
    "        players[agent_learner]='agent'\n",
    "        \n",
    "        current_state = None\n",
    "        A = None\n",
    "        \n",
    "        for j in range(9) : # The game takes at most 9 steps to finish\n",
    "            \n",
    "            #-- Agent plays\n",
    "            if env.current_player == agent_learner :                 \n",
    "                #-- Learning agent updates q_table\n",
    "                current_state = env.observe()[0]\n",
    "                current_state_hash = get_hash(current_state) \n",
    "                \n",
    "                # Add current_state in q_table if needed\n",
    "                if not(current_state_hash in q_table.keys()): \n",
    "                    q_table[current_state_hash]=np.zeros((1,9))                           \n",
    "                \n",
    "                A = get_action(epsilon(episode), current_state,q_table) #-- Choose action A with epsilon greedy\n",
    "                _,_,_ = env.step(A,print_grid=False) #-- Take action A \n",
    "                \n",
    "            #-- Optimal player plays \n",
    "            if not env.end :\n",
    "                grid,end,_ = env.observe() #-- observe grid\n",
    "                move = player_opt.act(grid) #-- get move\n",
    "                env.step(move,print_grid=False) # optimal player takes a move\n",
    "                \n",
    "                #-- update agent q_table after optimal player takes a step\n",
    "                if current_state is not None :\n",
    "                    agent_reward = env.reward(agent_learner)\n",
    "                    next_state = env.observe()[0]\n",
    "                    q_table = update_player(q_table,current_state,next_state,A,alpha,agent_reward,gamma) \n",
    "              \n",
    "            #-- Chek that the game has finished\n",
    "            if env.end :\n",
    "                \n",
    "                agent_reward = env.reward(agent_learner)\n",
    "                q_table = update_player(q_table,current_state,None,A,alpha,agent_reward,gamma) #-- Update q_table when game ends\n",
    "                \n",
    "                if env.winner is None :\n",
    "                    wins_count['draw'] = wins_count['draw'] + 1   \n",
    "                else :\n",
    "                    winner = players[env.winner]\n",
    "                    wins_count[winner] = wins_count[winner] + 1\n",
    "                    \n",
    "                if render : \n",
    "                    print(f\"Episode {episode} ; Winner is {winner}.\")\n",
    "                    env.render()\n",
    "\n",
    "                #-- accumulate rewards\n",
    "                accumulate_reward += env.reward(agent_learner)\n",
    "                \n",
    "                break\n",
    "                    \n",
    "            \n",
    "        if episode % 5000 == 0 :\n",
    "            print(f\"\\nEpisode : {episode}\")\n",
    "            print(wins_count)\n",
    "            \n",
    "    #--save\n",
    "    if path_save is not None:\n",
    "        with open(path_save,'wb') as file:\n",
    "            pickle.dump(q_table, file)\n",
    "            \n",
    "    #wandb.finish()\n",
    "    \n",
    "    return q_table,wins_count,agent_mean_rewards,M_opts,M_rands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Question 1\n",
    "eps_1=lambda x : 0.3\n",
    "if True:\n",
    "    env = TictactoeEnv()\n",
    "    q_table,wins_count,agent_mean_rewards,M_opts,M_rands = q_learning(epsilon=eps_1,num_episodes=int(20e3),eps_opt=0.5,\n",
    "                                                                      env=env,path_save=None,alpha=0.05,tag='Q.1',\n",
    "                                                                      gamma=0.99,render=False,test=False,wand_name='Tabular-Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Question 2 and 3\n",
    "eps_min=0.1\n",
    "eps_max=0.8\n",
    "if True :\n",
    "    env = TictactoeEnv()\n",
    "    test=True\n",
    "    for N_star in [1,10e3,20e3,30e3,40e3]:\n",
    "        print('--'*20,'>',N_star)\n",
    "        eps_2=lambda x : max([eps_min,eps_max*(1-x/N_star)])\n",
    "        q_table,wins_count,agent_mean_rewards,M_opts,M_rands = q_learning(epsilon=eps_2,num_episodes=int(20e3),env=env,tag='Q.2-Q.3',\n",
    "                                                                          wand_name=f'TabularQ--Nstar-{N_star}',\n",
    "                                                                          path_save=None,alpha=0.05,gamma=0.99,render=False,test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Additional : plotting epsilon profiles\n",
    "eps_min=0.1\n",
    "eps_max=0.8\n",
    "for N_star in [1,10e3,20e3,30e3,40e3]:\n",
    "    eps_2=lambda x : max([eps_min,eps_max*(1-x/N_star)])\n",
    "    plt.plot(list(map(eps_2,range(1,20000+1))),label=f'N_star {N_star}')\n",
    "plt.title(\"Epsilons\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Q.4 Select best N_star\n",
    "if True :\n",
    "    env = TictactoeEnv()\n",
    "    test=True\n",
    "    N_star = 10e3 # to update\n",
    "    Mopts=list()\n",
    "    Mrands=list()\n",
    "    for eps in [0,0.25,0.5,0.75,1]:\n",
    "        eps_2=lambda x : max([eps_min,eps_max*(1-x/N_star)])\n",
    "        q_table,wins_count,agent_mean_rewards,M_opts,M_rands = q_learning(epsilon=eps_2,eps_opt=eps,num_episodes=int(20e3),tag='Q.4--Nstar-{N_star}',wand_name=f'TabularQ--eps_opt-{eps}',\n",
    "                                                                          env=env,path_save=None,alpha=0.05,gamma=0.99,render=False,test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Q.5\n",
    "print(f\"The maximum M_opt is {max([max(a[1]) for a in data_q4.values()])}  \\nThe maximum M_rand is {max([max(a[0]) for a in data_q4.values()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning by self-practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define state table with all possible combinations.\n",
    "boards = []\n",
    "for i in range(0 , 19683) : \n",
    "    c=i\n",
    "    temp_boards = []\n",
    "    for ii in range(0 , 9) : \n",
    "        temp_boards.append(c % 3)\n",
    "        c = c // 3\n",
    "    boards.append(temp_boards)\n",
    "boards=np.array(boards)\n",
    "boards[boards == 0]= -1\n",
    "boards[boards == 2]= 0\n",
    "boards=boards.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boards' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb Cell 13'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000012vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdec_exp\u001b[39m(n, n_, e_min\u001b[39m=\u001b[39m\u001b[39m.1\u001b[39m, e_max\u001b[39m=\u001b[39m\u001b[39m.8\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000012vscode-remote?line=4'>5</a>\u001b[0m     \u001b[39m#helper function for decreasing exploration rate.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000012vscode-remote?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(e_min,e_max\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39m n\u001b[39m/\u001b[39mn_))\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000012vscode-remote?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state\u001b[39m(grid,player,boards\u001b[39m=\u001b[39mboards):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000012vscode-remote?line=9'>10</a>\u001b[0m     \u001b[39m#helper function to get the state from the grid. the sign changes for each player.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000012vscode-remote?line=10'>11</a>\u001b[0m     \u001b[39m#the state hash is index of the board in the boards table\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000012vscode-remote?line=11'>12</a>\u001b[0m     sign\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mX\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m1\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mO\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m}\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000012vscode-remote?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m boards\u001b[39m.\u001b[39mindex((sign[player\u001b[39m.\u001b[39mplayer]\u001b[39m*\u001b[39mgrid\u001b[39m.\u001b[39mflatten()\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mint32\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39mtolist())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'boards' is not defined"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "#-------------------------------------------- Helpers for self-learning\n",
    "\n",
    "def dec_exp(n, n_, e_min=.1, e_max=.8):\n",
    "    #helper function for decreasing exploration rate.\n",
    "    return max(e_min,e_max*(1- n/n_))\n",
    "\n",
    "\n",
    "def get_state(grid,player,boards=boards):\n",
    "    #helper function to get the state from the grid. the sign changes for each player.\n",
    "    #the state hash is index of the board in the boards table\n",
    "    sign={'X':1,'O':-1}\n",
    "    return boards.index((sign[player.player]*grid.flatten().astype('int32')).tolist())\n",
    "\n",
    "class Q_player:\n",
    "\n",
    "    def __init__(self,player='X', epsilon=0):\n",
    "\n",
    "        self.player=player\n",
    "        self.epsilon=epsilon #exploration rate\n",
    "        \n",
    "    def empty(self, grid):\n",
    "        #function taken from the tic_env.py provided script\n",
    "        #return all empty positions\n",
    "        grid=grid.flatten()\n",
    "        avail = []\n",
    "        for i in range(9):\n",
    "            if grid[i] == 0:\n",
    "                avail.append(i)\n",
    "        return avail\n",
    "\n",
    "    \n",
    "        \n",
    "    def act(self,state, grid, q_table):\n",
    "        avail=self.empty(grid)\n",
    "        \n",
    "        action_scores = q_table[state,:].copy()\n",
    "        \n",
    "        mask=np.ones(action_scores.shape,dtype=bool)\n",
    "        mask[avail]=False\n",
    "        action_scores[mask]= -np.inf    #not available positions Q_values are set to -inf\n",
    "                                        #so they will never be chosen by the agent\n",
    "        \n",
    "        action = np.random.choice(np.where(action_scores == np.max(action_scores))[0])\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(avail)\n",
    "        \n",
    "        return action\n",
    "\n",
    "def Metric_selfQ(policy,Q_table,optimal=False):\n",
    "    #computes the metrics from a given policy. optimal argument set the optimal or random opponent\n",
    "    N_wins=0\n",
    "    N_losses=0\n",
    "    N=0\n",
    "    Turns = np.array([['X','O']]*250+[['O','X']]*250)\n",
    "    for i in range(500):\n",
    "        np.random.seed()\n",
    "\n",
    "        \n",
    "        if optimal: \n",
    "            player_test = OptimalPlayer(epsilon=0., player=Turns[i,1])\n",
    "        if not optimal:\n",
    "            player_test = OptimalPlayer(epsilon=1., player=Turns[i,1])\n",
    "\n",
    "        player_new = policy(player=Turns[i,0],epsilon=0)\n",
    "        env=TictactoeEnv()\n",
    "        while not env.end:\n",
    "            if env.current_player == player_new.player:\n",
    "                state=get_state(env.grid,player_new)\n",
    "                move = player_new.act(state,env.grid,Q_table)       \n",
    "            else:\n",
    "                move = player_test.act(env.grid)\n",
    "\n",
    "            if not isinstance(move,tuple): \n",
    "                    move=(int(move/3),move%3)\n",
    "            env.step(move, print_grid=False)\n",
    "                \n",
    "        if env.winner==player_new.player:\n",
    "            N_wins+=1\n",
    "        if env.winner==player_test.player:\n",
    "            N_losses+=1\n",
    "        N+=1\n",
    "        env.reset()               \n",
    "    return (N_wins - N_losses)/N\n",
    "\n",
    "def play_game_selfQ(env,Q_table,p1,p2,alpha=0.1,gamma=0.99):\n",
    "    #play a self-learning game  and updates the Q-table\n",
    "    state1,new_state1,state2,new_state2=(None,None,None,None)\n",
    "\n",
    "    while not env.end:\n",
    "\n",
    "        if env.current_player == p1.player:\n",
    "\n",
    "            state1=get_state(env.grid,p1)\n",
    "            action1=p1.act(state1,env.grid,Q_table)\n",
    "\n",
    "            env.step(int(action1), print_grid=False)\n",
    "            if state2 is not None:\n",
    "                new_state2=get_state(env.grid,p2)\n",
    "                reward2=env.reward(p2.player)\n",
    "                Q_table[state2,action2] += alpha*(reward2 + gamma*(np.max(Q_table[new_state2,:])) - Q_table[state2,action2])\n",
    "                if env.end:\n",
    "                    reward1=env.reward(p1.player)\n",
    "                    Q_table[state1,action1] += alpha*(reward1 - Q_table[state1,action1])\n",
    "        else:\n",
    "            state2=get_state(env.grid,p2)\n",
    "            action2=p2.act(state2,env.grid,Q_table)\n",
    "            env.step(int(action2), print_grid=False)\n",
    "            if state1 is not None:\n",
    "                new_state1=get_state(env.grid,p1)\n",
    "                reward1=env.reward(p1.player)\n",
    "                Q_table[state1,action1] += alpha*(reward1 + gamma*(np.max(Q_table[new_state1,:])) - Q_table[state1,action1])\n",
    "\n",
    "            if env.end:\n",
    "                reward2=env.reward(p2.player)\n",
    "                Q_table[state2,action2] += alpha*(reward2 - Q_table[state2,action2])\n",
    "        \n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell trains the agent by self-pratice for 20'000 games\n",
    "\n",
    "decreasing_exp_rate=False #change this variable to true for decreasing exploration rate\n",
    "\n",
    "\n",
    "n_states=3**9\n",
    "n_actions=9\n",
    "M_opts=pd.DataFrame()\n",
    "M_rands=pd.DataFrame()\n",
    "exp_rates=[0,0.25,0.5,0.75,0.98]\n",
    "dec_speeds=[1,10000,20000,30000,40000]\n",
    "for i in range(len(exp_rates)):\n",
    "    M_opt=[]\n",
    "    M_rand=[]\n",
    "    p1=Q_player(player='X')\n",
    "    p2=Q_player(player='O')\n",
    "    lr=.05\n",
    "    Q_table=np.zeros((n_states,n_actions))\n",
    "    env=TictactoeEnv()\n",
    "    for game in tqdm_notebook(range(10),unit='game'):\n",
    "        if decreasing_exp_rate==False:\n",
    "            eps=exp_rates[i]\n",
    "        else:\n",
    "            eps=dec_exp(game, dec_speeds[i])\n",
    "        \n",
    "        p1.epsilon=eps    \n",
    "        p2.epsilon=eps\n",
    "        \n",
    "        if game%250==0:\n",
    "            #log metrics every 250 games\n",
    "            print(game,'eps',eps)\n",
    "            M_opt.append(Metric_selfQ(Q_player, Q_table, optimal=True))\n",
    "            M_rand.append(Metric_selfQ(Q_player, Q_table, optimal=False))\n",
    "            print(f'M_opt = {M_opt[-1]}, M_rand = {M_rand[-1]}')\n",
    "\n",
    "    \n",
    "        #play game\n",
    "        env_end=play_game_selfQ(env,Q_table,p1,p2,alpha=lr)\n",
    "        env.reset()\n",
    "\n",
    "    #log scores\n",
    "    if decreasing_exp_rate==False:   \n",
    "        M_opts[eps]=M_opt\n",
    "        M_rands[eps]=M_rand\n",
    "    else:\n",
    "        M_opts[dec_speeds[i]]=M_opt\n",
    "        M_rands[dec_speeds[i]]=M_rand\n",
    "\n",
    "    #save Q_table\n",
    "    Q_table_pd=pd.DataFrame(Q_table)\n",
    "    if decreasing_exp_rate==False:\n",
    "         Q_table_pd.to_pickle(f'Q_table_fixed_eps_{eps}.pkl')\n",
    "    else:\n",
    "        Q_table_pd.to_pickle(f'Q_table_dec_exp_nstar_{dec_speeds[i]}.pkl')\n",
    "#save scores in pickle format\n",
    "if decreasing_exp_rate==False:\n",
    "    M_opts.to_pickle('Q7_m_opts_fixed_eps.pkl')\n",
    "    M_rands.to_pickle('Q7_m_rands_fixed_eps.pkl')\n",
    "else:\n",
    "    M_opts.to_pickle('Q8_m_opts_dec_eps.pkl')\n",
    "    M_rands.to_pickle('Q8_m_rands_dec_eps.pkl')\n",
    "\n",
    "print('max m_rands: \\n',M_rands.max())\n",
    "print('max m_opts:\\n', M_opts.max())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "\n",
    "The policy update cells were adapted from the pytorch DQN tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Helpers ###################################\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN,self).__init__()\n",
    "\n",
    "        self.fc=nn.Sequential(nn.Linear(3*3*2,128),nn.ReLU(),\n",
    "                              nn.Linear(128,128),nn.ReLU(),\n",
    "                              nn.Linear(128,128),nn.ReLU(),\n",
    "                              nn.Linear(128,9))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.fc(x)\n",
    "        return x\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity,batch_size):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "        self.Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "        self.device='cpu'\n",
    "        self.batch_size=batch_size\n",
    "        self.step = 0\n",
    "        self.accumulated_loss = 0\n",
    "        self.optimizer = None\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(self.Transition(*args))\n",
    "\n",
    "    def sample(self):\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class deep_Q_player:\n",
    "    def __init__(self):\n",
    "        self.player=''\n",
    "        \n",
    "    def act(self,state,model:nn.Module,epsilon:float):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_scores = model(state)\n",
    "        \n",
    "        action = action_scores.max(1)[1].item()\n",
    "        \n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(range(9)).item()\n",
    "        \n",
    "        return action\n",
    "\n",
    "def grid2tensor(grid:np.array,player:str):\n",
    "    \n",
    "    state=np.zeros((3,3,2))\n",
    "    a = 2*(player=='X')-1 #  1 if player='X' and -1 otherwise\n",
    "    \n",
    "    grid1 = np.where(grid==a,1,0)\n",
    "    grid2 = np.where(grid==-a,1,0)\n",
    "    \n",
    "    state[:,:,0]=grid1\n",
    "    state[:,:,1]=grid2\n",
    "    state = torch.tensor(state)\n",
    "    \n",
    "    flatten_arrays = (state[:,:,0].flatten(),state[:,:,1].flatten())\n",
    "    \n",
    "    return torch.cat(flatten_arrays).view(-1,18).float()\n",
    "\n",
    "def test_policy(eps_optimalplayer,q_table=None,verbose=False,DQN_policy_net=None):\n",
    "    \n",
    "    env = TictactoeEnv() # environment\n",
    "    \n",
    "    if DQN_policy_net is None:\n",
    "        assert q_table is not None, \"Provide q_table\"\n",
    "        agent_player = agent(q_table) # agent    \n",
    "        \n",
    "    else:\n",
    "        agent_player=deep_Q_player()\n",
    "    \n",
    "    #-- Holders \n",
    "    wins_count = dict()\n",
    "    wins_count['optimal_player']=0\n",
    "    wins_count['agent']=0\n",
    "    wins_count['draw']=0 \n",
    "    players = dict()\n",
    "    players[None] = 'draw'\n",
    "    turns = np.array(['X','O'])\n",
    "    agent_symbol = None # 'X' or 'O'\n",
    "    optimal_symbol = None\n",
    "    num_illegal_steps = 0\n",
    "    \n",
    "    for episode in range(500):\n",
    "        \n",
    "        env.reset()\n",
    "        np.random.seed(episode) \n",
    "        \n",
    "        if episode < 250 :\n",
    "            agent_symbol = turns[0]\n",
    "            optimal_symbol = turns[1]\n",
    "            \n",
    "        else:\n",
    "            agent_symbol = turns[1]\n",
    "            optimal_symbol = turns[0]\n",
    "        \n",
    "        player_opt = OptimalPlayer(epsilon=eps_optimalplayer,player=optimal_symbol)\n",
    "        players[optimal_symbol]=(player_opt,'optimal_player')\n",
    "        players[agent_symbol]=(agent_player,'agent')\n",
    "        \n",
    "        for j in range(9):    \n",
    "            \n",
    "            #-- Get turn\n",
    "            turn = env.current_player\n",
    "            \n",
    "            #-- observe grid\n",
    "            grid,end,_ = env.observe() \n",
    "            \n",
    "            #-- Play\n",
    "            current_player, _ = players[turn]\n",
    "            \n",
    "            #-- Playing with DQN-agent\n",
    "            if DQN_policy_net is not None and turn==agent_symbol:\n",
    "                state = grid2tensor(grid,agent_symbol)\n",
    "                move = current_player.act(state,DQN_policy_net,0)\n",
    "                \n",
    "                try:\n",
    "                    env.step(move,print_grid=False)\n",
    "                    \n",
    "                except ValueError:\n",
    "                    env.end = True\n",
    "                    env.winner = optimal_symbol\n",
    "                    num_illegal_steps += 1\n",
    "   \n",
    "            else:\n",
    "                move = current_player.act(grid)  \n",
    "                env.step(move,print_grid=False)\n",
    "        \n",
    "            #-- Chek that the game has finished\n",
    "            if env.end :\n",
    "                if env.winner is not None :\n",
    "                    _,winner = players[env.winner]\n",
    "                    wins_count[winner] = wins_count[winner] + 1\n",
    "                else :\n",
    "                    wins_count['draw'] = wins_count['draw'] + 1\n",
    "                \n",
    "                break\n",
    "    \n",
    "    M = (wins_count['agent']-wins_count['optimal_player'])/500\n",
    "    \n",
    "    if verbose :\n",
    "        string =\"M_rand\"\n",
    "        if eps_optimalplayer < 1:\n",
    "            string = \"M_opt\"    \n",
    "        print(string+\" : \",M)\n",
    "        print(wins_count,'\\n')\n",
    "        print('Number of illegal steps',num_illegal_steps)\n",
    "\n",
    "    \n",
    "    return M,num_illegal_steps\n",
    "\n",
    "def update_policy(policy_net:nn.Module,\n",
    "                  target_net:nn.Module,\n",
    "                  memory:ReplayMemory,\n",
    "                  criterion=nn.SmoothL1Loss(),# F.huber_loss,\n",
    "                  gamma=0.99,\n",
    "                  online_update=False,online_state_action_reward=(None,None,None,None)):\n",
    "    \n",
    "    \n",
    "    if online_update :\n",
    "        #assert None not in online_state_action_reward,'provide these values.'\n",
    "        \n",
    "        #-- Compute Q values\n",
    "        state,next_state,action,reward = online_state_action_reward\n",
    "        state=state.to(memory.device)\n",
    "        state_action_values = policy_net(state)[:,action] # take Q(state,action)\n",
    "        \n",
    "        next_state_values=torch.tensor([0.0])\n",
    "        if next_state is not None:\n",
    "            next_state = next_state.to(memory.device)\n",
    "            next_state_values = target_net(next_state).max(1)[0].detach() # take max Q(state',action')\n",
    "        \n",
    "        #-- Compute target\n",
    "        target = reward + gamma*next_state_values\n",
    "        \n",
    "        #-- Update gradients\n",
    "        memory.optimizer.zero_grad()\n",
    "        loss = criterion(state_action_values,target)\n",
    "        loss.backward()\n",
    "        memory.optimizer.step\n",
    "        memory.step += 1\n",
    "\n",
    "        #-- Log\n",
    "        #wandb.log({'loss':loss.item(),'reward':reward,'Step':memory.step})\n",
    "\n",
    "    else:\n",
    "        if len(memory) < memory.batch_size:\n",
    "            return False\n",
    "        \n",
    "        #-- Sample Transitions\n",
    "        transitions = memory.sample()\n",
    "        \n",
    "        #-- GetTransition of batch-arrays\n",
    "        batch = memory.Transition(*zip(*transitions))\n",
    "        \n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=memory.device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        \n",
    "        state_batch = torch.cat(batch.state).to(memory.device)\n",
    "        action_batch = torch.cat(batch.action).to(memory.device)\n",
    "        reward_batch = torch.cat(batch.reward).to(memory.device)\n",
    "\n",
    "        #-- Compute Q values\n",
    "        memory.optimizer.zero_grad()\n",
    "        state_action_values = policy_net(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "        \n",
    "        #-- Compute target\n",
    "        next_state_values = torch.zeros(memory.batch_size,device=memory.device)\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "        target = reward_batch + gamma*next_state_values\n",
    "        \n",
    "        #-- Update gradients\n",
    "        loss = criterion(state_action_values,target.unsqueeze(1))#,reduction='mean')\n",
    "        loss.backward()\n",
    "        #for p in policy_net.parameters():\n",
    "        #    p.grad.data.clamp_(-1,1)\n",
    "        memory.optimizer.step()\n",
    "        memory.step += 1\n",
    "\n",
    "        #-- Log\n",
    "        #wandb.log({'loss':loss,'mean_batch_reward':reward_batch.float().mean(),'Step':memory.step})\n",
    "    \n",
    "    memory.accumulated_loss += loss.item()\n",
    "    return True # loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(epsilon,num_episodes:int,\n",
    "                    env:TictactoeEnv,\n",
    "                    path_save:str,\n",
    "                    eps_opt=0.5,gamma=0.99,\n",
    "                    render=False,test=False,online_update=False,wandb_tag=\"DQN\"):\n",
    "    #-- agent\n",
    "    agent = deep_Q_player()\n",
    "    \n",
    "    #-- Initialize Q networks\n",
    "    policy_net = DQN()\n",
    "    target_net = DQN()\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    #-- Initialize hyperparameters\n",
    "    batch_size=64\n",
    "    gamma=0.99\n",
    "    lr=5e-4\n",
    "    memory = ReplayMemory(10000,batch_size)\n",
    "    memory.optimizer = torch.optim.Adam(policy_net.parameters(),lr=lr) # optimizer\n",
    "    args={'gamma':gamma,'batch_size':batch_size,'replay_buffer':int(1e4),'lr':lr,'eps_opt':eps_opt,'online_update':online_update}\n",
    "    policy_net.to(memory.device)\n",
    "    target_net.to(memory.device)\n",
    "\n",
    "    #-- Holder \n",
    "    wins_count = dict()\n",
    "    wins_count['optimal_player']=0\n",
    "    wins_count['agent']=0\n",
    "    wins_count['draw']=0\n",
    "    players = dict()\n",
    "    M_opts = list()\n",
    "    M_rands = list()\n",
    "    accumulate_reward = 0\n",
    "    agent_mean_rewards = [0]*int(num_episodes//250)\n",
    "    num_illegal_actions = 0\n",
    "    turns = np.array(['X','O'])\n",
    "    \n",
    "    for episode in range(1,num_episodes+1):\n",
    "        \n",
    "        if episode % 250 == 0 :\n",
    "            agent_mean_rewards[int(episode//250)-1] = accumulate_reward/250\n",
    "            \n",
    "            accumulate_reward = 0 # reset\n",
    "            memory.accumulated_loss = 0 # reset\n",
    "            \n",
    "            if test:\n",
    "                M_opt,num_illegal_opt = test_policy(0,q_table=None,DQN_policy_net=policy_net,verbose=False)\n",
    "                M_rand,num_illegal_rand = test_policy(1,q_table=None,DQN_policy_net=policy_net,verbose=False)\n",
    "                M_opts.append(M_opt)\n",
    "                M_rands.append(M_rand)\n",
    "            \n",
    "        env.reset()\n",
    "        #-- Permuting player every 2 games\n",
    "        if episode % 2 == 0 :\n",
    "            turns[0] = 'X'\n",
    "            turns[1] = 'O'\n",
    "        else:\n",
    "            turns[0] = 'O'\n",
    "            turns[1] = 'X'\n",
    "        \n",
    "        player_opt = OptimalPlayer(epsilon=eps_opt,player=turns[0])\n",
    "        agent_learner = turns[1]\n",
    "        players[turns[0]]='optimal_player'\n",
    "        players[turns[1]]='agent'\n",
    "        \n",
    "        #--\n",
    "        current_state = None\n",
    "        A = None # action\n",
    "        \n",
    "        for j in range(9):\n",
    "  \n",
    "            #-- Agent plays \n",
    "            if env.current_player == turns[1] :\n",
    "                \n",
    "                current_state = grid2tensor(env.observe()[0],agent_learner)                          \n",
    "                A = agent.act(current_state, policy_net,epsilon(episode))  #----- Choose action A with epsilon greedy\n",
    "                #wandb.log({'action':A})  \n",
    "                \n",
    "                #----- Take action A\n",
    "                try :\n",
    "                    _,_,_ = env.step(A,print_grid=False)\n",
    "                                        \n",
    "                #----- End game when agent moves illegaly\n",
    "                except ValueError :                                \n",
    "                    num_illegal_actions += 1\n",
    "                    #wandb.log({'num_illegal_actions':num_illegal_actions})\n",
    "                    env.end = True #-- Terminating game\n",
    "                    env.winner = turns[0] # optimal player\n",
    "                    \n",
    "            #-- Optimal player plays \n",
    "            if not env.end :\n",
    "                \n",
    "                grid,end,_ = env.observe() #-- observe grid\n",
    "                move = player_opt.act(grid) #-- get move\n",
    "                env.step(move,print_grid=False) # optimal player takes a move\n",
    "  \n",
    "                #-- Update agent and Replay buffer\n",
    "                if current_state is not None :   \n",
    "                    next_state = grid2tensor(env.observe()[0],agent_learner)    \n",
    "                    agent_reward = env.reward(agent_learner)\n",
    "                    \n",
    "                    if not env.end : \n",
    "                        memory.push(current_state, torch.tensor([A]), next_state, torch.tensor([agent_reward]))\n",
    "                        \n",
    "                    if online_update:\n",
    "                        update_policy(policy_net,target_net,memory,gamma=gamma,\n",
    "                                      online_update=True,online_state_action_reward=(current_state,next_state,A,agent_reward))\n",
    "\n",
    "            #-- Update policy offline if applicable\n",
    "            if online_update == False :\n",
    "                success = update_policy(policy_net,target_net, memory,gamma=gamma, online_update=False)\n",
    "\n",
    "            #-- Chek that the game has finished\n",
    "            if env.end :\n",
    "                agent_reward = env.reward(agent_learner)\n",
    "                memory.push(current_state, torch.tensor([A]), None, torch.tensor([agent_reward])) #-- Store in Replay buffer\n",
    "                \n",
    "                if online_update:\n",
    "                    update_policy(policy_net,target_net,memory,gamma=gamma,\n",
    "                                  online_update=True,online_state_action_reward=(current_state,None,A,agent_reward))\n",
    "                #-- Logging\n",
    "                if env.winner is not None :\n",
    "                    winner = players[env.winner]\n",
    "                    wins_count[winner] = wins_count[winner] + 1            \n",
    "                else :\n",
    "                    wins_count['draw'] = wins_count['draw'] + 1  \n",
    "                accumulate_reward += agent_reward\n",
    "                \n",
    "                #-- Render\n",
    "                if render : \n",
    "                    print(f\"Episode {episode} ; Winner is {winner}.\")\n",
    "                    env.render()\n",
    "                    \n",
    "                break # stop for-loop\n",
    "        \n",
    "        #-- Log results            \n",
    "        if episode % 5000 == 0 :\n",
    "            print(f\"\\nEpisode : {episode}\")\n",
    "            print(wins_count)\n",
    "            \n",
    "        #-- Upddate target network\n",
    "        if episode % 500 == 0:\n",
    "            target_net.load_state_dict(deepcopy(policy_net.state_dict()))\n",
    "            target_net.eval()\n",
    "    \n",
    "\n",
    "    return wins_count,agent_mean_rewards,M_opts,M_rands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Q.11 & Q.12\n",
    "eps_1=lambda x : 0.3\n",
    "\n",
    "test=False\n",
    "for do in [False,True]:\n",
    "    env = TictactoeEnv()\n",
    "    wins_count,agent_mean_rewards,M_opts,M_rands = deep_q_learning(epsilon=eps_1,num_episodes=int(20e3),\n",
    "                                                                        eps_opt=0.5,env=env,path_save=None,\n",
    "                                                                        gamma=0.99,render=False,test=test,\n",
    "                                                                        wandb_tag=\"V3\",online_update=do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Q.13\n",
    "eps_min=0.1\n",
    "eps_max=0.8\n",
    "\n",
    "env = TictactoeEnv()\n",
    "test=True\n",
    "for N_star in [1,10e3,20e3,30e3,40e3]:\n",
    "    print('-'*20,' N_star : ',N_star,'-'*20)\n",
    "    eps_2=lambda x : max([eps_min,eps_max*(1-x/N_star)])\n",
    "    wins_count,agent_mean_rewards,M_opts,M_rands = deep_q_learning(epsilon=eps_2,num_episodes=int(20e3),\n",
    "                                                                        eps_opt=0.5,env=env,path_save=None,\n",
    "                                                                        gamma=0.99,render=False,test=test,\n",
    "                                                                        wandb_tag=f\"V3--{int(N_star)}\",online_update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Q.14\n",
    "eps_min=0.1\n",
    "eps_max=0.8\n",
    "\n",
    "env = TictactoeEnv()\n",
    "test=True\n",
    "N_star=1 # best N_star from Q.13\n",
    "eps_2=lambda x : max([eps_min,eps_max*(1-x/N_star)])\n",
    "for eps_opt in [0,0.25,0.5,0.75,1]:\n",
    "    print('-'*20,' eps_opt : ',eps_opt,'-'*20)\n",
    "    wins_count,agent_mean_rewards,M_opts,M_rands = deep_q_learning(epsilon=eps_2,num_episodes=int(20e3),\n",
    "                                                                        eps_opt=eps_opt,env=env,path_save=None,\n",
    "                                                                        gamma=0.99,render=False,test=test,\n",
    "                                                                        wandb_tag=f\"V3--eps_opt:{eps_opt}\",online_update=False)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory_self(object):\n",
    "\n",
    "    def __init__(self,capacity):\n",
    "        self.memory=deque([],maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "    def __getitem__(self,idx):\n",
    "        return self.memory[idx]\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "Transition = namedtuple('T',\n",
    "                        ('player','state', 'action','validity','reward','new_state'))\n",
    "def dec_exp(n, n_, e_min=.1, e_max=.8):\n",
    "    #helper function for decreasing exploration rate.\n",
    "    return max(e_min,e_max*(1- n/n_))\n",
    "\n",
    "def dec_exp(n, n_, e_min=.1, e_max=.8):\n",
    "    #helper function for decreasing exploration rate.\n",
    "    return max(e_min,e_max*(1- n/n_))\n",
    "    \n",
    "def Metric_deep_Q_self(policy,model,optimal=False):\n",
    "    N_wins=0\n",
    "    N_losses=0\n",
    "    N=0\n",
    "    Turns = np.array([['X','O']]*250+[['O','X']]*250)\n",
    "    for i in range(500):\n",
    "        np.random.seed()\n",
    "\n",
    "        \n",
    "        if optimal: \n",
    "            player_test = OptimalPlayer(epsilon=0., player=Turns[i,1])\n",
    "        if not optimal:\n",
    "            player_test = OptimalPlayer(epsilon=1., player=Turns[i,1])\n",
    "\n",
    "        player_new = policy(player=Turns[i,0],epsilon=0)\n",
    "        env=TictactoeEnv()\n",
    "        while not env.end:\n",
    "            if env.current_player == player_new.player:\n",
    "                state=grid2tensor_self(env.grid,player_new.player)\n",
    "                move = player_new.act(state,model)  \n",
    "                if not env.check_valid((int(move/3), move % 3)):\n",
    "                    env.winner=player_test.player\n",
    "                    env.end=True\n",
    "                    break \n",
    "            else:\n",
    "                move = player_test.act(env.grid)\n",
    "\n",
    "            if not isinstance(move,tuple): \n",
    "                    move=(int(move/3),move%3)\n",
    "            env.step(move, print_grid=False)\n",
    "                \n",
    "        if env.winner==player_new.player:\n",
    "            N_wins+=1\n",
    "        if env.winner==player_test.player:\n",
    "            N_losses+=1\n",
    "        N+=1\n",
    "        env.reset()               \n",
    "    return (N_wins - N_losses)/N\n",
    "\n",
    "class deep_Q_player_self:\n",
    "    def __init__(self,player='X', epsilon=.0):\n",
    "        self.player=player\n",
    "        self.epsilon=epsilon\n",
    "    \n",
    "        \n",
    "    def act(self,state,model):\n",
    "        \n",
    "        action = model(state).max(1)[1].view(-1,1)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = torch.tensor([[random.randrange(9)]])\n",
    "        \n",
    "        return action\n",
    "\n",
    "\n",
    "def grid2tensor_self(grid,player):\n",
    "    tens=torch.zeros(3,3,2)\n",
    "    grid=torch.from_numpy(grid)\n",
    "    if player=='X':\n",
    "        player_index=torch.nonzero(grid==1,as_tuple=False)\n",
    "        op_index=torch.nonzero(grid==-1,as_tuple=False)\n",
    "    if player=='O':\n",
    "        player_index=torch.nonzero(grid==-1,as_tuple=False)\n",
    "        op_index=torch.nonzero(grid==1,as_tuple=False)\n",
    "    tens[player_index[:,0],player_index[:,1],0]=1\n",
    "    tens[op_index[:,0],op_index[:,1],1]=1\n",
    "    return tens\n",
    "\n",
    "def tensor2grid(tensor,player='X'):\n",
    "\n",
    "    if tensor is None:\n",
    "        return None\n",
    "    grid_new=torch.zeros(3,3)\n",
    "    player='X'\n",
    "    if player=='X':\n",
    "        player_index=torch.nonzero(tensor[:,:,0]==1,as_tuple=False)\n",
    "        op_index=torch.nonzero(tensor[:,:,1]==1,as_tuple=False)\n",
    "    if player=='O':\n",
    "        player_index=torch.nonzero(grid==-1,as_tuple=False)\n",
    "        op_index=torch.nonzero(grid==1,as_tuple=False)\n",
    "    grid_new[player_index[:,0],player_index[:,1]]=1\n",
    "    grid_new[op_index[:,0],op_index[:,1]]=-1\n",
    "    return grid_new.int().numpy()\n",
    "\n",
    "\n",
    "def play_game_self_deepQ(env,model,p1,p2):\n",
    "    R_t=[]\n",
    "    while not env.end:\n",
    "        if env.current_player == p1.player:\n",
    "            state1=grid2tensor(env.grid,p1.player)\n",
    "            action1=p1.act(state1,model)\n",
    "            if env.end: break\n",
    "            if env.check_valid((int(action1/3), action1 % 3)):\n",
    "                env.step((int(action1/3), action1 % 3), print_grid=False)\n",
    "                reward1=torch.tensor([env.reward(p1.player)])\n",
    "                new_state1=grid2tensor_self(env.grid,p1.player)\n",
    "                R_t.append((state1,action1,reward1,new_state1))\n",
    "            else:\n",
    "                reward1=torch.tensor([-1])\n",
    "                R_t.append((state1,action1,reward1,torch.tensor([])))\n",
    "                break\n",
    "\n",
    "        if env.current_player == p2.player:\n",
    "            state2=grid2tensor(env.grid,p2.player)\n",
    "            action2=p2.act(state2,model)\n",
    "            if env.end: break\n",
    "            if env.check_valid((int(action2/3), action2 % 3)):\n",
    "                env.step((int(action2/3), action2 % 3), print_grid=False)\n",
    "                reward2=torch.tensor([env.reward(p2.player)])\n",
    "                new_state2=grid2tensor(env.grid,p2.player)\n",
    "                R_t.append((state2,action2,reward2,new_state2))\n",
    "            else:\n",
    "                reward2=torch.tensor([-1])\n",
    "                R_t.append((state2,action2,reward2,torch.tensor([])))\n",
    "                break\n",
    "        \n",
    "    return env, R_t\n",
    "\n",
    "def render(grid):\n",
    "    if grid is None:\n",
    "        print(None)\n",
    "        return\n",
    "    # print current grid\n",
    "    value2player = {0: '-', 1: 'X', -1: 'O'}\n",
    "    for i in range(3):\n",
    "        print('|', end='')\n",
    "        for j in range(3):\n",
    "            print(value2player[int(grid[i,j])], end=' ' if j<2 else '')\n",
    "        print('|')\n",
    "class DQN_self(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN_self,self).__init__()\n",
    "\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(3*3*2,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,9)\n",
    "            )\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x=x.view(-1,18)\n",
    "        x=self.fc(x)\n",
    "        return x   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8 game 0\n",
      "M_opt = -1.0, M_rand = -1.0\n",
      "0.78 game 250\n",
      "M_opt = -1.0, M_rand = 0.316\n",
      "0.76 game 500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb Cell 24'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000023vscode-remote?line=65'>66</a>\u001b[0m \u001b[39mif\u001b[39;00m test:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000023vscode-remote?line=66'>67</a>\u001b[0m     \u001b[39mprint\u001b[39m(eps,\u001b[39m'\u001b[39m\u001b[39mgame\u001b[39m\u001b[39m'\u001b[39m,game)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000023vscode-remote?line=67'>68</a>\u001b[0m     m_opt\u001b[39m.\u001b[39mappend(Metric_deep_Q_self(deep_Q_player_self,policy,optimal\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000023vscode-remote?line=68'>69</a>\u001b[0m     m_rand\u001b[39m.\u001b[39mappend(Metric_deep_Q_self(deep_Q_player_self,policy,optimal\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)) \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000023vscode-remote?line=69'>70</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mM_opt = \u001b[39m\u001b[39m{\u001b[39;00mm_opt[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, M_rand = \u001b[39m\u001b[39m{\u001b[39;00mm_rand[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb Cell 23'\u001b[0m in \u001b[0;36mMetric_deep_Q_self\u001b[0;34m(policy, model, optimal)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000022vscode-remote?line=43'>44</a>\u001b[0m \u001b[39mif\u001b[39;00m env\u001b[39m.\u001b[39mcurrent_player \u001b[39m==\u001b[39m player_new\u001b[39m.\u001b[39mplayer:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000022vscode-remote?line=44'>45</a>\u001b[0m     state\u001b[39m=\u001b[39mgrid2tensor_self(env\u001b[39m.\u001b[39mgrid,player_new\u001b[39m.\u001b[39mplayer)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000022vscode-remote?line=45'>46</a>\u001b[0m     move \u001b[39m=\u001b[39m player_new\u001b[39m.\u001b[39;49mact(state,model)  \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000022vscode-remote?line=46'>47</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m env\u001b[39m.\u001b[39mcheck_valid((\u001b[39mint\u001b[39m(move\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m), move \u001b[39m%\u001b[39m \u001b[39m3\u001b[39m)):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000022vscode-remote?line=47'>48</a>\u001b[0m         env\u001b[39m.\u001b[39mwinner\u001b[39m=\u001b[39mplayer_test\u001b[39m.\u001b[39mplayer\n",
      "\u001b[1;32m/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb Cell 23'\u001b[0m in \u001b[0;36mdeep_Q_player_self.act\u001b[0;34m(self, state, model)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000022vscode-remote?line=71'>72</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mact\u001b[39m(\u001b[39mself\u001b[39m,state,model):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000022vscode-remote?line=73'>74</a>\u001b[0m     action \u001b[39m=\u001b[39m model(state)\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000022vscode-remote?line=74'>75</a>\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandom() \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000022vscode-remote?line=75'>76</a>\u001b[0m         action \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[random\u001b[39m.\u001b[39mrandrange(\u001b[39m9\u001b[39m)]])\n",
      "File \u001b[0;32m~/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb Cell 23'\u001b[0m in \u001b[0;36mDQN_self.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000022vscode-remote?line=166'>167</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000022vscode-remote?line=167'>168</a>\u001b[0m     x\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m18\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000022vscode-remote?line=168'>169</a>\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(x)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tcastigl/ANN/miniproject_ANN/Miniproject1_full_code.ipynb#ch0000022vscode-remote?line=169'>170</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=96'>97</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=97'>98</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/ANN/DLenv/lib/python3.8/site-packages/torch/nn/functional.py:1299\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/functional.py?line=1296'>1297</a>\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/functional.py?line=1297'>1298</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/functional.py?line=1298'>1299</a>\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m   <a href='file:///home/tcastigl/ANN/DLenv/lib/python3.8/site-packages/torch/nn/functional.py?line=1299'>1300</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "decreasing_exp_rate=True #change this variable to true for decreasing exploration rate\n",
    "\n",
    "\n",
    "\n",
    "exp_rates=[0.1,0.25,0.5,0.75,0.98]\n",
    "dec_speeds=[10000,20000,30000,40000]\n",
    "\n",
    "\n",
    "M_rands=pd.DataFrame()\n",
    "M_opts=pd.DataFrame()\n",
    "\n",
    "for i in range(len(exp_rates)):\n",
    "    \n",
    "    policy=DQN_self()\n",
    "    target=DQN_self()\n",
    "    target.load_state_dict(policy.state_dict()) #target is initialized as a copy of policy\n",
    "    target.eval()\n",
    "    criterion=torch.nn.HuberLoss()\n",
    "    optimizer=torch.optim.Adam(policy.parameters(),lr=5e-4)\n",
    "    gamma=.99\n",
    "    batch_size=64\n",
    "    game=0\n",
    "\n",
    "    if decreasing_exp_rate==False:\n",
    "        eps=exp_rates[i]\n",
    "    else:\n",
    "        eps=dec_exp(game,dec_speeds[i])\n",
    "\n",
    "    p1=deep_Q_player_self('X',epsilon=eps)\n",
    "    p2=deep_Q_player_self('O',epsilon=eps)\n",
    "\n",
    "    env=TictactoeEnv()\n",
    "    \n",
    "    state1, state2, next_state1, next_state2=(None,None,None,None)\n",
    "\n",
    "    m_rand=[]\n",
    "    m_opt=[]\n",
    "    losses=[]\n",
    "    updates=0\n",
    "    test,update=(False,False)\n",
    "    t_test,t_updt=(0,0)\n",
    "\n",
    "    R=ReplayMemory_self(10000)\n",
    "    \n",
    "    while game <=2000:\n",
    "        \n",
    "        #update exploration rates\n",
    "        if decreasing_exp_rate==False:\n",
    "            eps=exp_rates[i]\n",
    "        else:\n",
    "            eps=dec_exp(game,dec_speeds[i])\n",
    "        \n",
    "        p1.epsilon=eps\n",
    "        p2.epsilon=eps\n",
    "\n",
    "        #test and update conditions setup\n",
    "        if (game-1)%250==0: \n",
    "            t_test=0\n",
    "        if (game%250==0) & (t_test==0):\n",
    "            test=True\n",
    "        if (game-1)%250==0: \n",
    "            t_updt=0\n",
    "        if (game%250==0) & (t_updt==0):\n",
    "            update=True\n",
    "            \n",
    "        if test:\n",
    "            print(eps,'game',game)\n",
    "            m_opt.append(Metric_deep_Q_self(deep_Q_player_self,policy,optimal=True))\n",
    "            m_rand.append(Metric_deep_Q_self(deep_Q_player_self,policy,optimal=False)) \n",
    "            print(f'M_opt = {m_opt[-1]}, M_rand = {m_rand[-1]}')\n",
    "            t_test+=1\n",
    "            test=False\n",
    "\n",
    "        #-------------------------play action--------------------------------#\n",
    "        invalid=False   #bool variable to know when an illegal move is done\n",
    "\n",
    "        if env.current_player==p1.player:\n",
    "            \n",
    "            state1=grid2tensor_self(env.grid,player=p1.player)\n",
    "            action1=p1.act(state1,model=policy)\n",
    "            \n",
    "            if env.check_valid(int(action1)):   #make move only if valid\n",
    "                env.step(int(action1))\n",
    "                next_state2=grid2tensor_self(env.grid,player=p2.player)   #next_state2 is computed when it is its turn to play\n",
    "\n",
    "                if state2 is not None:\n",
    "                    reward2=torch.tensor([env.reward(p2.player)]) \n",
    "                    R.push(2,state2,action2,'valid',reward2,next_state2)\n",
    "                    \n",
    "                    \n",
    "            else:\n",
    "                #handling invalid moves\n",
    "                env.end=True\n",
    "                invalid=True\n",
    "\n",
    "                reward1=torch.tensor([-1])\n",
    "                R.push(1,state1,action1,'invalid',reward1,None)\n",
    "                \n",
    "                reward2=torch.tensor([env.reward(p2.player)])\n",
    "                next_state2=None\n",
    "                R.push(2,state2,action2,'valid',reward2,next_state2)\n",
    "            \n",
    "\n",
    "        if (env.current_player==p2.player) & (not env.end):\n",
    "            \n",
    "            state2=grid2tensor_self(env.grid,player=p2.player)\n",
    "            action2=p2.act(state2,policy)\n",
    "            \n",
    "            if env.check_valid(int(action2)):   #make move only if valid\n",
    "                env.step(int(action2))\n",
    "                next_state1=grid2tensor_self(env.grid,player=p1.player) #next_state2 is computed when it is its turn to play\n",
    "\n",
    "            \n",
    "                reward1=torch.tensor([env.reward(p1.player)])\n",
    "                R.push(1,state1,action1,'valid',reward1,next_state1)\n",
    "                \n",
    "            else:\n",
    "                env.end=True\n",
    "                invalid=True\n",
    "\n",
    "                reward2=torch.tensor([-1])\n",
    "                R.push(2,state2,action2,'invalid',reward2,None)\n",
    "                \n",
    "                reward1=torch.tensor([env.reward(p1.player)])\n",
    "                next_state1=None\n",
    "                R.push(1,state1,action1,'valid',reward1,next_state1)\n",
    "\n",
    "\n",
    "        if env.end & (invalid==False):\n",
    "            \n",
    "            #tie\n",
    "            if env.winner==None:\n",
    "                if env.current_player==p1.player:\n",
    "                    next_state2=None\n",
    "                else: \n",
    "                    next_state1=None\n",
    "                reward1=torch.tensor([env.reward(p1.player)]) #should be 0\n",
    "                reward2=torch.tensor([env.reward(p2.player)]) #should be 0\n",
    "                R.push(1,state1,action1,'valid',reward1,next_state1)\n",
    "                R.push(2,state2,action2,'valid',reward2,next_state2)\n",
    "                \n",
    "            #one winner\n",
    "            else:\n",
    "                if env.winner == p1.player:\n",
    "                    next_state1=None\n",
    "                if env.winner == p2.player:\n",
    "                    next_state2=None\n",
    "\n",
    "                reward1=torch.tensor([env.reward(p1.player)])\n",
    "                reward2=torch.tensor([env.reward(p2.player)])\n",
    "                R.push(1,state1,action1,'valid',reward1,next_state1)\n",
    "                R.push(2,state2,action2,'valid',reward2,next_state2)\n",
    "                \n",
    "\n",
    "        if env.end:\n",
    "            state1, state2, next_state1, next_state2=(None,None,None,None)\n",
    "            env.reset()\n",
    "            game+=1\n",
    "            \n",
    "#-------------------------------------update policy----------------------------------------#            \n",
    "        if len(R) > 64:\n",
    "            sample=R.sample(batch_size)\n",
    "            sample=Transition(*zip(*sample))\n",
    "\n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                                sample.new_state)))\n",
    "            non_final_new_states = torch.cat([s for s in sample.new_state\n",
    "                                                if s is not None])\n",
    "            states=torch.cat([state.unsqueeze(0)for state in sample.state])\n",
    "            actions=torch.cat(sample.action)\n",
    "            rewards=torch.cat(sample.reward)\n",
    "            \n",
    "            s_a_vals=policy(states).gather(1, actions)\n",
    "            \n",
    "            new_state_vals=torch.zeros(batch_size)\n",
    "            new_state_vals[non_final_mask]=target(non_final_new_states).max(1)[0]\n",
    "\n",
    "            \n",
    "            expected=new_state_vals*gamma+rewards\n",
    "            loss=criterion(s_a_vals,expected.unsqueeze(1))\n",
    "            losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if update:  #update target\n",
    "            target.load_state_dict(policy.state_dict())\n",
    "            t_updt+=1\n",
    "            update=False\n",
    "            updates+=1\n",
    "\n",
    "    if decreasing_exp_rate==False:   \n",
    "        M_opts[eps]=m_opt\n",
    "        M_rands[eps]=m_rand\n",
    "    else:\n",
    "        M_opts[dec_speeds[i]]=m_opt\n",
    "        M_rands[dec_speeds[i]]=m_rand\n",
    "\n",
    "    #save policy\n",
    "    if decreasing_exp_rate==False:\n",
    "         torch.save(policy.state_dict(),f'DQN_self_lr_fixed_{eps}.pkl')\n",
    "    else:\n",
    "        torch.save(policy.state_dict(),f'DQN_self_lr_decreasing_{dec_speeds[i]}.pkl')\n",
    "\n",
    "#save scores in pickle format\n",
    "if decreasing_exp_rate==False:\n",
    "    M_opts.to_pickle('Q16_m_opts_fixed_eps.pkl')\n",
    "    M_rands.to_pickle('Q16_m_rands_fixed_eps.pkl')\n",
    "else:\n",
    "    M_opts.to_pickle('Q17_m_opts_dec_eps.pkl')\n",
    "    M_rands.to_pickle('Q17_m_rands_dec_eps.pkl')\n",
    "\n",
    "print('max m_rands: \\n',M_rands.max())\n",
    "print('max m_opts:\\n', M_opts.max())\n",
    "    #bug wtf"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "316bcdbfefbf4057bea60a74d4981cf3304ddbec1a2ad80e595554335e51b39e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('DLenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
